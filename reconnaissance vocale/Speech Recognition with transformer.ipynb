{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e1da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec29de0",
   "metadata": {},
   "source": [
    "# COUCHE D ENTREE DU DECODEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efd7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super()._init__()\n",
    "        self.emb= tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_em= tf.keras.layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "        \n",
    "    def call(self, x):\n",
    "        maxlen= tf.shape(x)[-1]\n",
    "        x= self.emb(x)\n",
    "        \n",
    "        positions= tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions=self.pos_emb(positions)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed5d15",
   "metadata": {},
   "source": [
    "# COUCHE D ENTREE DE L ENCODEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187bf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356855f7",
   "metadata": {},
   "source": [
    "# COUCHE D ENCODEUR DU TRANSFORMATEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranssformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.01):\n",
    "        super().__init__()\n",
    "        self.att= layers.MultiHeadAttention(num_heads=num_heads, key_dim= embed_dim)\n",
    "        self.ffn= tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(ffn_dim, activation=\"relu\")\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ]\n",
    "        )\n",
    "        self.layernorm1= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1= tf.keras.Dropout(rate)\n",
    "        self.dropout2= tf.keras.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs,training):\n",
    "        att_output= self.att(inputs, inputs)\n",
    "        att_output= self.dropout1(att_output, training= training)\n",
    "        \n",
    "        out1= self.layernorm1(inputs+att_output)\n",
    "        ffn_output= self.ffn(out1)\n",
    "        ffn_output= self.dropout(ffn_output, training= training)\n",
    "        return self.layernorm2(out1+ffn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12eea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self , embed_dim, num_heads, ffn_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.self_att= tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.enc_att= tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout= tf.keras.layers.Dropout(0.5)\n",
    "        self.enc_dropout= tf.keras.layers.Dropout(0.1)\n",
    "        self.ffn_dropout= tf.keras.layers.Dropout(0.1)\n",
    "        ffn= tf.keras.Sequential(\n",
    "        [\n",
    "           \n",
    "            tf.keras.layers.Dense(ffn_dim, actvation=\"relu\")\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        \n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def call( self, enc_out, target):\n",
    "        input_shape= tf.shape(target)\n",
    "        batch_size= input_shape[0]\n",
    "        seq_len= input_shape[1]\n",
    "        causal_mask= self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att=self.self_att(target, target, attention_mask= causal_mask)\n",
    "        target_norm= self.layernorm1(target+ self.self_dropout(target_att))\n",
    "        enc_out= self.enc_att(target_norm , enc_out)\n",
    "        enc_out_norm= self.layernorm2(self.enc_dropout(enc_out)+target_norm)\n",
    "        ffn_out= self.ffn(enc_out_norm)\n",
    "        ffn_out_norm= self.layernorm3(enc_out_norm+ self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b5c63",
   "metadata": {},
   "source": [
    "# TRANSFORMER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895040a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(self,\n",
    "                num_hid=64,\n",
    "                num_head=2,\n",
    "                num_ff=128,\n",
    "                source_maxlen=100,\n",
    "                target_maxlen=100,\n",
    "                num_layers_enc=4,\n",
    "                num_layers_dec=1,\n",
    "                num_classes=10,):\n",
    "        super().__init__()\n",
    "        self.loss_metric= tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc= num_layers_enc\n",
    "        self.num_layers_dec= num_layers_dec\n",
    "        self.target_maxlen=target_maxlen\n",
    "        self.num_classes= num_classes\n",
    "        \n",
    "        self.enc_input= SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input= TokenEmbedding( num_vocab=num_classes, maxlen=target_maxlen , num_hid= num_hid)\n",
    "        \n",
    "        #define the encoder laer of transformer\n",
    "        self.encoder= tf.keras.Sequential(\n",
    "        [self.enc_input]+[TransformerEncoder(num_hid, num_head, num_ff) for _ in range(num_layers_enc)])\n",
    "        \n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, batch , idx_to_token, target_start_token_idx=27, target_end_token_idx=28):\n",
    "        self.batch=batch\n",
    "        self.target_start_token_idx= target_start_token_idx\n",
    "        self.target_end_token_idx= target_end_token_idx\n",
    "        self.idx_to_char= idx_to_token\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch%5!=0:\n",
    "            return\n",
    "        source=self.batch[\"source\"]\n",
    "        target=self.batch[\"target\"].numpy()\n",
    "        bs= tf.shape(source)[0]\n",
    "        preds= self.model.generate(source, self.target_start_token_idx)\n",
    "        preds= preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text= \"\".join([self.idx_to_char[_] for _ in range(target[i,:])])\n",
    "            prediction=\"\"\n",
    "            for idx in preds[i,:]:\n",
    "                prediction+= self.idx_to_char[idx]\n",
    "                if idx== self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:   {target_text.replace(\"-\", \"\")})\n",
    "            print(f\"predictions:  {predictionss}\\n)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02248fe",
   "metadata": {},
   "source": [
    "# BAREME D 'APPRENTISSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 init_lr=0.00001 ,\n",
    "                 lr_after_warnup=0.001 ,\n",
    "                 final_lr=0.00001, \n",
    "                 warnup_epochs=15,\n",
    "                 decay_epochs=85,\n",
    "                steps_per_epoch=203,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.init_lr= init_lr\n",
    "        self.lr_after_warnup= lr_after_warnup\n",
    "        self.final_lr=final_lr\n",
    "        self.warnup_epochs= warnup_epochs\n",
    "        self.decay_epochs=decay_epochs\n",
    "        self.steps_per_epoch= steps_per_epoch\n",
    "        \n",
    "    def calculate_lr(self, epoch):\n",
    "        warnup_lr=(\n",
    "        self.init_lr\n",
    "            +((self.lr_after_warnup- self.init_lr) / (self.warnup_epochs-1))*epoch)\n",
    "        \n",
    "        decay_lr=tf.math.maximum(self.final_lr, self.after_warnup-(epoch - self.warnup_epochs)\n",
    "                                *(self.lr_after_warnup- self.final_lr)/(self.decay_epochs),)\n",
    "        \n",
    "        return tf.math.minimum(warnup_lr , decay_lr)\n",
    "        \n",
    "\n",
    "\n",
    "def call(self , step):\n",
    "    epoch= step// self.steps_per_epoch\n",
    "    return self.calculate_lr(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a63e5",
   "metadata": {},
   "source": [
    "# CREER ET FORMER LE MODELE DE BOUT EN BOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76822521",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch= next(iter(val_ds))\n",
    "\n",
    "idx_to_char= vectorizer.get_vocabulary()\n",
    "display_cb= DisplayOutputs(batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3 )\n",
    "\n",
    "Model= Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400, \n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=34)\n",
    "\n",
    "loss_fn= tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1,)\n",
    "\n",
    "learning_rate=CustomSchedule(\n",
    "    init_lr=0.00001,\n",
    "    lr_after_warnup=0.001,\n",
    "    final_lr=0.00001,\n",
    "    warnup_epochs=15,\n",
    "    decay_epochs=85,\n",
    "    steps_per_epoch=len(ds),)\n",
    "optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "Model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "history= Model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epoch=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd95d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb8c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed68710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c70bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83443bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
